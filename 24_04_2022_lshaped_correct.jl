using JuMP
using NLPModelsJuMP, NLPModels
using GLPK
using Distributions
using Random
using DataFrames
using CSV
using Printf

function print_iteration(k, args...)
    f(x) = Printf.@sprintf("%12.4e", x)
    println(lpad(k, 9), " ", join(f.(args), " "))
    return
end

###############################################################
# PARAMETERS
###############################################################
H = 4
J = 3
Táµˆ = 1
TË¢ = 1
Î = 1


s = [1.0  0.0  0.0;  0.0  1.0  0.0; 0.0  0.0  1.0; 1.0  0.0  0.0]

d = [
    0.7  0.8  0.8  0.6  0.9  1.0  1.0  0.9  0.0  1.0
    0.2  0.2  1.0  0.9  0.7  0.7  0.4  1.0  0.3  0.7
    0.5  0.6  0.5  0.1  0.4  0.6  0.4  0.0  0.5  0.6
    0.5  1.4  0.3  1.0  0.8  0.8  0.2  0.8  0.6  0.6
    0.8  0.2  1.3  0.6  0.3  0.4  0.6  0.8  0.4  0.8
    0.3  0.4  0.9  0.2  0.6  1.0  0.4  0.3  0.4  0.6
]


Random.seed!(1234)

#Î¼ = [1.2, 1.8, 1.6]
#Î£ = [0.5 0.0 0.0; 0.0 0.5 0.0; 0.0 0.0 0.5]
#dÎ¾ = reshape(rand(MvNormal(Î¼,Î£), Î ), (J,TË¢,Î) )
#dÎ¾ = round.(dÎ¾ , digits = 2)

dÎ¾ = [1.2 , 1.8 , 3.2]

Ï‡áµáµƒË£ = fill(1, H ,J)


záµ– = [7; 9; 11;7 ] * 1000
zá¶œ = záµ– *2

záµ = fill(0, H, J)

for h=1
        záµ[h , 2] = (1/2) * záµ–[h]
        záµ[h , 3] = (3/4) * záµ–[h]
end

for h=2
        záµ[h , 3] = (1/2) * záµ–[h]
        záµ[h , 1] = (3/4) * záµ–[h]
end

for h=3
        záµ[h , 1] = (1/2) * záµ–[h]
        záµ[h , 2] = (3/4) * záµ–[h]
end

for h=4
        záµ[h , 2] = (1/2) * záµ–[h]
        záµ[h , 3] = (3/4) * záµ–[h]
end

;





#########################################
# main model
#########################################

main = Model(GLPK.Optimizer)
@variable(main, Ïˆ[1:H , 1:J], Bin)
@variable(main, 0 â‰¤ Ï‡[1:H , 1:J])
@variable(main, Î±[1:H , 1:J , 1:Táµˆ], Bin)
@variable(main, -1000000 â‰¤ Î¸)

@variable(main, z1[1:H , 1:J , 1:Táµˆ], Bin)
@variable(main, z2[1:H , 1:J , 1:Táµˆ])

@objective(main, Min, sum( záµ–[h] * Ïˆ[h,j] * (Táµˆ + TË¢) + záµ[h,j] * Ï‡[h,j] 
                           for h in 1:H for j in 1:J) + Î¸);


#con1_s1 = @NLconstraint(main, demand_met[j in 1:J , t in 1:Táµˆ], 
#                          sum(  Î±[h,j,t] *(Ïˆ[h,j] + Ï‡[h,j]) for h in 1:H ) â‰¥ d[j,t] );

#################
# LINEARIZING
#################
@constraint(main, linear1[h = 1:H , j = 1:J , t =1:Táµˆ], z1[h,j,t] â‰¤ Î±[h,j,t] )
@constraint(main, linear2[h = 1:H , j = 1:J , t =1:Táµˆ], z1[h,j,t] â‰¤ Ïˆ[h,j] )
@constraint(main, linear3[h = 1:H , j = 1:J , t =1:Táµˆ], z1[h,j,t] â‰¥ Î±[h,j,t] + Ïˆ[h,j] -1 )

@constraint(main, linear4[h = 1:H , j = 1:J , t =1:Táµˆ], z2[h,j,t] â‰¤ Î±[h,j,t] )
@constraint(main, linear5[h = 1:H , j = 1:J , t =1:Táµˆ], z2[h,j,t] â‰¤ Ï‡[h,j] )
@constraint(main, linear6[h = 1:H , j = 1:J , t =1:Táµˆ], z2[h,j,t] â‰¥ Î±[h,j,t] + Ï‡[h,j] -1 )

con1_s1 = @constraint(main, demand_met[ j in 1:J , t in 1:Táµˆ ], 
    sum(z1[h,j,t] + z2[h,j,t] for h in 1:H ) â‰¥ d[j,t] );

con2_s1 = @constraint(main, permanent_pool[h in 1:H , j in 1:J], Ïˆ[h,j] â‰¤ s[h,j]);

con3_s1 = @constraint(main, allocation_recruitment_training[h in 1:H , j in 1:J , t in 1:Táµˆ],
                      Î±[h,j,t] â‰¤ Ïˆ[h,j] + 10 * Ï‡[h,j] );

con4_s1 = @constraint(main, no_more_than_one_station[h in 1:H , t in 1:Táµˆ],
              sum(Î±[h,j,t] for j in 1:J) â‰¤ 1  )

con4_s1 = @constraint(main, single_skilling_at_recruitment[h in 1:H , j in 1:J], 
    Ï‡[h,j] â‰¤ ( 1 - Ïˆ[h,j] ) * Ï‡áµáµƒË£[h,j] );

con5_s1 = @constraint(main, training_recruited[ h in 1:H ], 
     sum(Ï‡[h,j] for j in 1:J ) â‰¤ sum(Ïˆ[h,j] for j in 1:J) * J  );


################################################
# sub solve   outout-> Î± Î³ o
################################################

function sub_solve(Ïˆ , Ï‡)
    sub = Model(GLPK.Optimizer)
    @variable(sub, Î±[1:H , 1:J , 1:TË¢ , 1:Î], Bin )
    @variable(sub, 0 â‰¤ Î³[1:J , 1:TË¢ , 1:Î]);
    @objective(sub, Min,  (1/Î) * sum(zá¶œ[j] * Î³[j,t,Î¾] for j in 1:J for t in 1:TË¢ for Î¾ in 1:Î) )
    con1_S2 = @constraint(sub, demand_met[ j in 1:J , t in 1:TË¢ , Î¾ in 1:Î],
     sum(Î±[h,j,t,Î¾] * (Ïˆ[h,j] + Ï‡[h,j]) for h in 1:H ) + Î³[j,t,Î¾] â‰¥ dÎ¾[j,t,Î¾]  );
    con2_s2 = @constraint(sub, permanent_allocability[h in 1:H , j in 1:J , t in 1:TË¢ , Î¾ in 1:Î],
            Î±[h,j,t,Î¾] â‰¤ Ïˆ[h,j] + 10 * Ï‡[h,j] );
    con3_s2 = @constraint(sub, no_more_than_one_station[h in 1:H , t in 1:TË¢ , Î¾ in 1:Î],
           sum( Î±[h,j,t,Î¾] for j in 1:J) â‰¤ 1 );
    optimize!(sub)
    Î± = value.(Î±)
    Î³ = value.(Î³)
    o = objective_value(sub)
    Î± = round.(Î± , digits = 2)
    Î³ = round.(Î³ , digits = 2)
    o = round.(o , digits = 2)
    return (Î± , Î³ , o )
end


################################################################
# sub_dual -> Î»
###########################################
# when we want to get the dual the x (first stage variables are considered as fixed)
# when we want to get the coefficents of x, x should be variable
# therefore, we need to define two models one for dual and one for coefficient
###########################################
###############################################################

function sub_dual(Ïˆ , Ï‡ , Î±áµ)
    sub = Model(GLPK.Optimizer) 
    @variable(sub,  Î±[1:H, 1:J, 1:TË¢ , 1:Î])
    @variable(sub, 0 â‰¤ Î³[1:J , 1:TË¢ , 1:Î])
    @objective(sub, Min,  sum(zá¶œ[j] * Î³[j,t,Î¾] for j in 1:J for t in 1:TË¢ for Î¾ in 1:Î ) )
    for h in 1:H
            for j in 1:J
                for t in 1:TË¢
                    for Î¾ in 1:Î
                        if (Î±áµ[h , j , 1 , 1] == 0 )
                            con = @constraint(sub, Î±[h,j,t,Î¾] == 0)
                        else
                            con = @constraint(sub, Î±[h,j,t,Î¾] == 1)
                        end
                    end
                end
            end
        end

    con1_S2 = @constraint(sub, demand_met[ j in 1:J , t in 1:TË¢ , Î¾ in 1:Î],
         dÎ¾[j,t,Î¾] â‰¤ sum(Î±[h,j,t,Î¾] * (Ïˆ[h,j] + Ï‡[h,j]) for h in 1:H ) + Î³[j,t,Î¾]   )
    con2_s2 = @constraint(sub, permanent_allocability[h in 1:H , j in 1:J , t in 1:TË¢ , Î¾ in 1:Î],
                Î±[h,j,t,Î¾] â‰¤ Ïˆ[h,j] + 10 * Ï‡[h,j] );
    con3_s2 = @constraint(sub, no_more_than_one_station[h in 1:H , t in 1:TË¢ , Î¾ in 1:Î],
               sum( Î±[h,j,t,Î¾] for j in 1:J) â‰¤ 1 );
    optimize!(sub)

    con_equal = all_constraints(sub, AffExpr, MOI.EqualTo{Float64})
    con_less = all_constraints(sub, AffExpr, MOI.LessThan{Float64})
    Î»1 = dual.(con_equal)
    Î»2 = dual.(con_less)
    Î» = append!(Î»1 , Î»2)

    no_con_equal = length(con_equal)
    no_con_less = length(con_less)
    no_all_con = no_con_equal + no_con_less;
    #@show no_con_equal
    #@show no_con_less
    #@show no_all_con;
    
    return Î»
end



#############################################
# function for coefficients of  ğœ“  and  ğœ’
#############################################

function sub_coeff(Î±áµ)  
    sub = Model(GLPK.Optimizer) 
    @variable(sub, Ïˆ[1:H , 1:J], Bin )
    @variable(sub, 0 â‰¤ Ï‡[1:H , 1:J])
    @variable(sub,  Î±[1:H, 1:J , 1:TË¢, 1:Î])
    @variable(sub, 0 â‰¤ Î³[1:J , 1:TË¢ , 1:Î])
     @objective(sub, Min,  sum(zá¶œ[j] * Î³[j,t,Î¾] for j in 1:J for t in 1:TË¢ for Î¾ in 1:Î ) )
    for h in 1:H
            for j in 1:J
                for t in 1:TË¢
                    for Î¾ in 1:Î
                        if Î±áµ[h , j , 1,  1] == 0
                            con = @constraint(sub, Î±[h,j,t,Î¾] == 0)
                        else
                            con = @constraint(sub, Î±[h,j,t,Î¾] == 1)
                        end
                    end
                end
            end
        end

    con1_S2 = @NLconstraint(sub, demand_met[ j in 1:J , t in 1:TË¢ , Î¾ in 1:Î],
         dÎ¾[j,t,Î¾] â‰¤ sum(Î±[h,j,t,Î¾] * (Ïˆ[h,j] + Ï‡[h,j]) for h in 1:H ) + Î³[j,t,Î¾]   )
    con2_s2 = @constraint(sub, permanent_allocability[h in 1:H , j in 1:J , t in 1:TË¢ , Î¾ in 1:Î],
                Î±[h,j,t,Î¾] â‰¤ Ïˆ[h,j] + 10 * Ï‡[h,j] );
    con3_s2 = @constraint(sub, no_more_than_one_station[h in 1:H , t in 1:TË¢ , Î¾ in 1:Î],
               sum( Î±[h,j,t,Î¾] for j in 1:J) â‰¤ 1 );

    vr = all_variables(sub)
    vr_index = [vr[i].index.value for i in 1:length(vr)]
    df = DataFrame(variable = vr , index = vr_index); 
    #@show df
    
    nlp = MathOptNLPModel(sub)
    q = zeros(nlp.meta.nvar)
    jac(nlp, q)
    A1 = jac(nlp, q)[ : , 1:24]
    return A1
end


println("        k   lower bound  upper bound   gap")
function initiate()
    for k in 1:10
        optimize!(main)
        Ïˆáµ = value.(Ïˆ)
        Ï‡áµ = value.(Ï‡)
        x = all_variables(main)[1 : (H *J*2) ]
        lb = objective_value(main)
        sub_solve(Ïˆáµ , Ï‡áµ)
        Î±áµ = sub_solve(Ïˆáµ , Ï‡áµ)[1]
        Î³áµ = sub_solve(Ïˆáµ , Ï‡áµ)[2]
        oáµ = sub_solve(Ïˆáµ , Ï‡áµ)[3]
        Î»áµ = sub_dual(Ïˆáµ , Ï‡áµ , Î±áµ)
        A1 = sub_coeff(Î±áµ)
        ub = sum( záµ–[h] * Ïˆáµ[h,j] * (Táµˆ + TË¢) + záµ[h,j] * Ï‡áµ[h,j] for h in 1:H for j in 1:J) + 
             sum(zá¶œ[j] * Î³áµ[j,t,Î¾] for j in 1:J for t in 1:TË¢ for Î¾ in 1:Î ) 
        gap = (ub - lb ) / ub
        print_iteration(k , lb , ub, gap)
        if gap < 0.000001
            println("************************************")
            println("***     we are at optimality   ***  ")
            break
        end
        #############################
        cut = @constraint(main, Î¸ â‰¥ oáµ - (Î»áµ)' * A1 * (x .- value.(x) ) )
        println("we are adding the cut $(cut)")
    end
end

initiate()
